{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"fb15k\", \"fb15k237\", \"pathqueryFB\", \"pathqueryWN\", \"wn18\", \"wn18rr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing KBC Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_entities_relations(train, valid, test):\n",
    "    entity_list = {}\n",
    "    relation_list = {}\n",
    "    \n",
    "    for input_file in [train, valid, test]:\n",
    "        print(f\"Working with {input_file} file\")\n",
    "        # tab-separated (head, relation, tail) triples\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                tokens = line.strip().split(\"\\t\")\n",
    "                assert len(tokens) == 3\n",
    "                entity_list[tokens[0]] = len(entity_list)\n",
    "                entity_list[tokens[2]] = len(entity_list)\n",
    "                relation_list[tokens[1]] = len(relation_list)\n",
    "\n",
    "    return entity_list, relation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(vocabulary, entity_list, relation_list):\n",
    "    fout = open(vocabulary, \"w\")\n",
    "    fout.write(\"[PAD]\" + \"\\n\")\n",
    "    for i in range(95):\n",
    "        fout.write(f\"[unused{i}]\\n\")\n",
    "    fout.write(\"[UNK]\" + \"\\n\")\n",
    "    fout.write(\"[CLS]\" + \"\\n\")\n",
    "    fout.write(\"[SEP]\" + \"\\n\")\n",
    "    fout.write(\"[MASK]\" + \"\\n\")\n",
    "    for e in entity_list.keys():\n",
    "        fout.write(e + \"\\n\")\n",
    "    for r in relation_list.keys():\n",
    "        fout.write(r + \"\\n\")\n",
    "    vocab_size = 100 + len(entity_list) + len(relation_list)\n",
    "    print(f\"Vocabulary size {vocab_size}\")\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    fin = open(vocab_file)\n",
    "    for num, line in enumerate(fin):\n",
    "        items = line.strip().split(\"\\t\")\n",
    "        if len(items) > 2:\n",
    "            break\n",
    "        token = items[0]\n",
    "        index = items[1] if len(items) == 2 else num\n",
    "        token = token.strip()\n",
    "        vocab[token] = int(index)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_true_triples(train, valid, test, vocab, output_file):\n",
    "    true_triples = []\n",
    "    for input_file in [train, valid, test]:\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                h, r, t = line.strip('\\r \\n').split('\\t')\n",
    "                assert (h in vocab) and (r in vocab) and (t in vocab)\n",
    "                hpos = vocab[h]\n",
    "                rpos = vocab[r]\n",
    "                tpos = vocab[t]\n",
    "                true_triples.append((hpos, rpos, tpos))\n",
    "    \n",
    "    print(f\"Number of true triples: {len(true_triples)}\")\n",
    "    fout = open(output_file, \"w\")\n",
    "    for hpos, rpos, tpos in true_triples:\n",
    "        fout.write(str(hpos) + \"\\t\" + str(rpos) + \"\\t\" + str(tpos) + \"\\n\")\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask_type(input_file, output_file):\n",
    "    with open(output_file, \"w\") as fw:\n",
    "        with open(input_file, \"r\") as fr:\n",
    "            for line in fr.readlines():\n",
    "                fw.write(line.strip('\\r \\n') + \"\\tMASK_HEAD\\n\")\n",
    "                fw.write(line.strip('\\r \\n') + \"\\tMASK_TAIL\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbc_data_preprocess(old_train, old_valid, old_test, \n",
    "                        vocabulary, triples_file, \n",
    "                        new_train, new_valid, new_test):\n",
    "    \n",
    "    print(\"Extracting unique entities and relations...\")\n",
    "    entity_list, relation_list = get_unique_entities_relations(old_train, old_valid, old_test)\n",
    "    \n",
    "    print(\"Updating vocabulary...\")\n",
    "    write_vocab(vocabulary, entity_list, relation_list)\n",
    "    \n",
    "    # OrderedDict vocab\n",
    "    vocab = load_vocab(vocabulary)\n",
    "    \n",
    "    print(\"Writing triples...\")\n",
    "    write_true_triples(old_train, old_valid, old_test, vocab, triples_file)\n",
    "\n",
    "    print(\"Generating masks...\")\n",
    "    generate_mask_type(old_train, new_train)\n",
    "    generate_mask_type(old_valid, new_valid)\n",
    "    generate_mask_type(old_test, new_test)\n",
    "    \n",
    "    print(\"Preprocessing successful!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbc_datasets = [\"fb15k\", \"fb15k237\", \"wn18\", \"wn18rr\"]\n",
    "data = \"data\\\\fb15k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing (input) files\n",
    "old_train = os.path.join(os.getcwd(), data, \"train.txt\")\n",
    "old_valid = os.path.join(os.getcwd(), data, \"valid.txt\")\n",
    "old_test = os.path.join(os.getcwd(), data, \"test.txt\")\n",
    "\n",
    "# new (output) file\n",
    "vocabulary = os.path.join(os.getcwd(), data, \"vocab.txt\")\n",
    "triples_file = os.path.join(os.getcwd(), data, \"all.txt\")\n",
    "new_train = os.path.join(os.getcwd(), data, \"train.coke.txt\")\n",
    "new_valid = os.path.join(os.getcwd(), data, \"valid.coke.txt\")\n",
    "new_test = os.path.join(os.getcwd(), data, \"test.coke.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unique entities and relations...\n",
      "Working with D:\\Workspace\\_RWTH\\_msc_mi\\SoSe 2022\\XAI\\CoKE\\CoKE\\data\\fb15k\\train.txt file\n",
      "Working with D:\\Workspace\\_RWTH\\_msc_mi\\SoSe 2022\\XAI\\CoKE\\CoKE\\data\\fb15k\\valid.txt file\n",
      "Working with D:\\Workspace\\_RWTH\\_msc_mi\\SoSe 2022\\XAI\\CoKE\\CoKE\\data\\fb15k\\test.txt file\n",
      "Updating vocabulary...\n",
      "Vocabulary size 16396\n",
      "Writing triples...\n",
      "Number of true triples: 592213\n",
      "Generating masks...\n",
      "Preprocessing successful!!\n"
     ]
    }
   ],
   "source": [
    "kbc_data_preprocess(old_train, old_valid, old_test, \n",
    "                    vocabulary, triples_file, \n",
    "                    new_train, new_valid, new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Path Query Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathquery_get_unique_entities_relations(train, valid, test):\n",
    "    entity_list = {}\n",
    "    relation_list = {}\n",
    "    \n",
    "    for input_file in [train, valid, test]:\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                tokens = line.strip().split(\"\\t\")\n",
    "                assert len(tokens) == 3\n",
    "                entity_list[tokens[0]] = len(entity_list)\n",
    "                entity_list[tokens[2]] = len(entity_list)\n",
    "                relations = tokens[1].split(\",\")\n",
    "                for relation in relations:\n",
    "                    relation_list[relation] = len(relation_list)\n",
    "    \n",
    "    return entity_list, relation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_base_data(old_train, old_valid, old_test,\n",
    "                     train_base, valid_base, test_base):\n",
    "    def fil_base(input_file, output_file):\n",
    "        fout = open(output_file, \"w\")\n",
    "        base_n = 0\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                tokens = line.strip().split(\"\\t\")\n",
    "                assert len(tokens) == 3\n",
    "                relations = tokens[1].split(\",\")\n",
    "                if len(relations) == 1:\n",
    "                    fout.write(line)\n",
    "                    base_n += 1\n",
    "        fout.close()\n",
    "        return base_n\n",
    "\n",
    "    train_base_n = fil_base(old_train, train_base)\n",
    "    valid_base_n = fil_base(old_valid, valid_base)\n",
    "    test_base_n = fil_base(old_test, test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onlytail_mask_type(input_file, output_file):\n",
    "    with open(output_file, \"w\") as fw:\n",
    "        with open(input_file, \"r\") as fr:\n",
    "            for line in fr.readlines():\n",
    "                fw.write(line.strip('\\r \\n') + \"\\tMASK_TAIL\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathquery_data_preprocess(old_train, old_valid, old_test,\n",
    "                              vocab_path, sen_candli_file, trivial_sen_file,\n",
    "                              new_train, new_valid, new_test,\n",
    "                              train_base, valid_base, test_base):\n",
    "    \n",
    "    print(\"Extracting unique entities and relations...\")\n",
    "    entity_list, relation_list = pathquery_get_unique_entities_relations(old_train, old_valid, old_test)\n",
    "    \n",
    "    print(\"Updating vocabulary...\")\n",
    "    write_vocab(vocab_path, entity_list, relation_list)\n",
    "    \n",
    "    filter_base_data(old_train, old_valid, old_test,\n",
    "                     train_base, valid_base, test_base)\n",
    "    \n",
    "    generate_mask_type(old_train, new_train)\n",
    "    generate_onlytail_mask_type(old_valid, new_valid)\n",
    "    generate_onlytail_mask_type(old_test, new_test)\n",
    "    \n",
    "    vocab = load_vocab(vocab_path)\n",
    "    \n",
    "#     generate_eval_files(vocab_path, old_test, \n",
    "#                         train_base, valid_base, test_base, \n",
    "#                         sen_candli_file, trivial_sen_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathquery_datasets = [\"pathqueryFB\", \"pathqueryWN\"]\n",
    "data = \"data\\\\pathqueryFB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing (input) files\n",
    "old_train = os.path.join(os.getcwd(), data, \"train\")\n",
    "old_valid = os.path.join(os.getcwd(), data, \"valid\")\n",
    "old_test = os.path.join(os.getcwd(), data, \"test\")\n",
    "\n",
    "new_train = os.path.join(os.getcwd(), data, \"train.coke.txt\")\n",
    "new_valid = os.path.join(os.getcwd(), data, \"valid.coke.txt\")\n",
    "new_test = os.path.join(os.getcwd(), data, \"test.coke.txt\")\n",
    "\n",
    "vocab_file = os.path.join(os.getcwd(), data, \"vocab.txt\")\n",
    "sen_candli_file = os.path.join(os.getcwd(), data, \"sen_candli.txt\")\n",
    "trivial_sen_file = os.path.join(os.getcwd(), data, \"trivial_sen.txt\")\n",
    "\n",
    "train_base = os.path.join(os.getcwd(), data, \"train.base.txt\")\n",
    "valid_base = os.path.join(os.getcwd(), data, \"valid.base.txt\")\n",
    "test_base = os.path.join(os.getcwd(), data, \"test.base.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pathquery_data_preprocess(old_train, old_valid, old_test,\n",
    "#                               vocab_file, sen_candli_file, trivial_sen_file,\n",
    "#                               new_train, new_valid, new_test,\n",
    "#                               train_base, valid_base, test_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
